{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sentence Categorization.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Gw_6jrFvuXZV","colab_type":"text"},"source":["# Cài đặt môi trường"]},{"cell_type":"code","metadata":{"id":"v7HXNQlUViix","colab_type":"code","colab":{}},"source":["# Để tách từ, dùng 1 trong 2 thư viện pyvi, hoặc underthesea\n","# Cài underthesea\n","!pip install underthesea\n","\n","# Cài pyvi\n","!pip install pyvi"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6yi2lEYXN4n7","colab_type":"code","colab":{}},"source":["# Xin quyền ghi Google drive (ghi model)\n","print(\"\\n\\nXin quyền ghi Google drive (ghi model)\")\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","BASE_FOLDER_NAME = \"/content/drive/My Drive/Research/NLP/NLP Journalism/RESULT/\"\n","DATA_FOLDER_NAME = BASE_FOLDER_NAME + \"CSV files/\"\n","CLASSIFICATION_FOLDER_NAME = BASE_FOLDER_NAME"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2-Ml7OMIuh-E","colab_type":"code","colab":{}},"source":["import numpy as np\n","from scipy import spatial\n","import pandas as pd\n","import csv\n","\n","from ast import literal_eval\n","from underthesea import sent_tokenize\n","from underthesea import word_tokenize\n","from pyvi import ViTokenizer\n","\n","import enum\n","import string\n","import time\n","\n","from sklearn.cluster import KMeans\n","\n","\n","class TOKENIZER(enum.Enum): \n","    underthesea = 1\n","    pyvi = 2\n","\n","def tokenize(sentence, type = TOKENIZER.pyvi):\n","  vec = []\n","  if type == TOKENIZER.underthesea: # sử dụng thư viện tách từ underthesea\n","    vec = word_tokenize(sentence)\n","    vec = [word.replace(\" \",\"_\").lower() for word in vec]\n","  if type == TOKENIZER.pyvi: # sử dụng thư viện tách từ pyvi\n","    vec = ViTokenizer.tokenize(sentence)\n","    vec = [word.lower() for word in vec.split(\" \")]\n","  return vec\n","\n","class SentenceClassification:\n","  def __init__(self, sentences, tokenized_sentences, word_model, stop_words, remove_stop_words = True):\n","    self.sentences = sentences\n","    self.tokenized_sentences = tokenized_sentences\n","    self.word_model = word_model\n","    self.stop_words = stop_words\n","    self.remove_stop_words = remove_stop_words\n","\n","    self.index2word_set = set(self.word_model.wv.index2word)\n","    self.calculate_feature_vectors()\n","\n","  def avg_feature_vector(self, tokenized_sentence):\n","    feature_vec = np.zeros((self.word_model.vector_size, ), dtype='float32')\n","    n_words = 0\n","    for word in tokenized_sentence:\n","      if (not self.remove_stop_words) or (self.remove_stop_words and word not in self.stop_words):\n","        if word in self.index2word_set:\n","          n_words += 1\n","          feature_vec = np.add(feature_vec, self.word_model[word])\n","    if (n_words > 0):\n","      feature_vec = np.divide(feature_vec, n_words)\n","    feature_vec = np.append(feature_vec, n_words) # Thêm chiều số từ\n","    return feature_vec\n","\n","  def calculate_feature_vectors(self):\n","    start_time = time.time()\n","    self.feature_vectors = np.zeros(shape=(len(self.tokenized_sentences), self.word_model.vector_size + 1)) # Cộng 1, vì đã thêm chiều số từ\n","    for i in range(len(self.tokenized_sentences)):\n","      self.feature_vectors[i] = self.avg_feature_vector(self.tokenized_sentences[i])\n","    print(\"--- Calculating features takes %s seconds ---\" % (time.time() - start_time))\n","    return self.feature_vectors\n","  \n","  def sim(self, s1, s2):\n","    s1_afv = self.avg_feature_vector(s1)\n","    s2_afv = self.avg_feature_vector(s2)\n","    sim = 1 - spatial.distance.cosine(s1_afv, s2_afv)\n","    return sim\n","  \n","  def classify(self, level=[100]):\n","    keys = [] # Mảng các tập keys cấp 1, 2, 3, ... Sẽ có dạng: [[\"0\", \"1\", ...], [\"0-0\", \"0-1\", ... \"1-0\", \"1-1\", ...], [\"0-0-0\", \"0-0-1\", ...]]\n","    \n","    clusters = {} # Lưu kết quả phân cụm các câu đầu vào\n","    clusters1 = self._classify(range(len(self.sentences)), level[0]) # Phân cụm lần 1\n","\n","    keysLevel1 = [] # Tập key cấp 1, sẽ có dạng: [\"0\", \"1\", \"2\", ...]\n","    for i in range(len(clusters1)):\n","      key = str(i)\n","      keysLevel1.append(key)\n","      clusters[key] = clusters1[i] # Lưu lại nhóm câu cấp 1: clusters[\"0\"], cluster[\"1\"], ...\n","    keys.append(keysLevel1) # Lưu tập key cấp 1 vào mảng tập keys\n","\n","    for i in range(1, len(level)):\n","      keysLeveli = [] # Tập key cấp i\n","      for previousKey in keys[i-1]:\n","        print(previousKey)\n","        indexes = clusters[previousKey] # Tập index của các câu cần phân cụm tiếp (index tính theo mảng câu ban đầu)\n","        if len(indexes) == 0: # Nếu tập các câu cần phân cụm tiếp là tập rỗng, bỏ qua\n","          pass\n","        else:\n","          clustersi = self._classify(indexes, level[i]) # Phân cụm câu\n","          for j in range(len(clustersi)):\n","            key = previousKey + \"-\" + str(j) # key cấp i được phát triển từ key cấp i-1\n","            keysLeveli.append(key) \n","            clusters[key] = clustersi[j] # Lưu lại nhóm câu cấp i, theo tập key cấp i\n","      keys.append(keysLeveli) # Lưu tập key cấp i vào tập keys\n","    \n","    clusters[\"keys\"] = keys\n","    self.clusters = clusters\n","    return clusters\n","\n","\n","  def _classify(self, indexes, n_clusters = -1):\n","    # Step 1: Tổng hợp features\n","    if len(indexes) == len(self.sentences):\n","      X = self.feature_vectors\n","    else:\n","      X = np.zeros(shape=(len(indexes), self.word_model.vector_size + 1)) # Cộng 1, vì đã thêm chiều số từ\n","      for i in range(len(indexes)):\n","        X[i] = self.feature_vectors[indexes[i]]\n","\n","    n_clusters = min(len(indexes), n_clusters)\n","\n","    # Step 2: Clustering, using KMeans\n","    start_time = time.time()\n","    if n_clusters > 0:\n","      kmeans = KMeans(n_clusters = n_clusters, random_state=0).fit(X)\n","    else:\n","      kmeans = KMeans(random_state=0).fit(X)\n","    print(\"--- Kmeans takes %s seconds ---\" % (time.time() - start_time))\n","    \n","    # Step 3: Build up returned result\n","    n_clusters= kmeans.n_clusters\n","    pred_label = kmeans.predict(X)\n","\n","    clusters = []\n","    for i in range(0, n_clusters):\n","      clusters.append([])\n","    for i in range(0, pred_label.size):\n","      clusters[pred_label[i]].append(indexes[i])\n","    \n","    return clusters\n","\n","class ModelTrain:\n","  def __init__(self, tokenized_sentences, file_path_to_save_model = \"\"):\n","    self.tokenized_sentences = tokenized_sentences\n","    self.file_path_to_save_model = file_path_to_save_model\n","  \n","  def train(self):\n","    print('\\nTraining word2vec...')\n","    start_time = time.time()\n","    word_model = Word2Vec(self.tokenized_sentences, size=200, min_count=1, window=5, iter=150)\n","    pretrained_weights = word_model.wv.syn0\n","    vocab_size, emdedding_size = pretrained_weights.shape\n","    print('Result embedding shape:', pretrained_weights.shape)\n","    if self.file_path_to_save_model != \"\":\n","      word_model.wv.save_word2vec_format(self.file_path_to_save_model, binary = True)\n","    print(\"--- %s seconds ---\" % (time.time() - start_time))\n","    return word_model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AKj-hgIW1Vep","colab_type":"text"},"source":["# Prepare sentences"]},{"cell_type":"code","metadata":{"id":"4hzgQYV2O59K","colab_type":"code","outputId":"920a6432-2715-4417-b8f8-2a136baf4d04","executionInfo":{"status":"ok","timestamp":1584981137201,"user_tz":-420,"elapsed":5657,"user":{"displayName":"Dat Trinh Tuan","photoUrl":"","userId":"16424334760389233527"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["SAVED_SENTENCES_FILE_PATH = CLASSIFICATION_FOLDER_NAME + \"00Vnexpress.Sentences.txt\"\n","SAVED_TOKENIZED_SENTENCES_FILE_PATH = CLASSIFICATION_FOLDER_NAME + \"00Vnexpress.Sentences.Tokenized.txt\"\n","FILE_NAME = \"Vnexpress.Articles.CSV\"\n","\n","def prepareSentences():\n","  df = pd.read_csv(DATA_FOLDER_NAME + FILE_NAME, encoding='utf-8', sep=',', index_col=0)\n","\n","  # Loại bỏ lỗi duplicate bài viết (link khác nhau, nội dung, thời gian giống nhau)\n","  print(\"Kích thước trước khi xử lý duplicate: \", df.shape)\n","  df.drop_duplicates(subset=[\"title\", \"time\"], keep = \"first\", inplace = True)\n","  print(\"Kích thước sau khi xử lý duplicate: \", df.shape)\n","\n","  # df = df.iloc[:1000]\n","\n","  print(df.shape)\n","\n","  global all_sentences\n","  all_sentences = []\n","  global all_tokenized_sentences\n","  all_tokenized_sentences = []\n","  start_time = time.time()\n","  for index, row in df.iterrows():\n","      # print(row[\"link\"])\n","      try:\n","        article = row['content']\n","        paragraphs = literal_eval(article)\n","        translator=str.maketrans('','',string.punctuation + \"0123456789\")\n","        for paragraph in paragraphs:\n","            sentences = sent_tokenize(paragraph)\n","            for sentence in sentences:\n","              tmp = sentence.translate(translator)\n","              if tmp != \"\":\n","                all_sentences.append(sentence)\n","                vec = tokenize(tmp)\n","                all_tokenized_sentences.append(vec)\n","                all\n","      except Exception as ex:\n","        print(\"\\nERROR with %s. Reason: %s\" %(paragraphs, str(ex)))\n","  print(\"--- %s seconds ---\" % (time.time() - start_time))\n","\n","\n","import pickle\n","def saveSentences():\n","  with open(SAVED_SENTENCES_FILE_PATH, \"wb\") as fp: # Pickling\n","    pickle.dump(all_sentences, fp)\n","  with open(SAVED_TOKENIZED_SENTENCES_FILE_PATH, \"wb\") as fp: # Pickling\n","    pickle.dump(all_tokenized_sentences, fp)\n","def load_sentences(path):\n","  with open(path, \"rb\") as fp: # Unpickling\n","    sentences = pickle.load(fp)\n","  print('Num sentences:', len(sentences))\n","  return sentences\n","\n","\n","prepareSentences()\n","saveSentences()\n","# all_sentences = load_sentences(SAVED_SENTENCES_FILE_PATH)\n","# all_tokenized_sentences = load_sentences(SAVED_TOKENIZED_SENTENCES_FILE_PATH)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Kích thước trước khi xử lý duplicate:  (16100, 6)\n","Kích thước sau khi xử lý duplicate:  (12544, 6)\n","(1000, 6)\n","--- 3.76168155670166 seconds ---\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"i2KJ4Ei0Vnyb","colab_type":"text"},"source":["# Sentences filtering"]},{"cell_type":"markdown","metadata":{"id":"BqyB7LQm9R_w","colab_type":"text"},"source":["### Vn-index"]},{"cell_type":"code","metadata":{"id":"Q1G45MPvVxEz","colab_type":"code","colab":{}},"source":["import re\n","\n","for i in range(len(all_sentences) - 1, -1, -1):\n","  if not re.search(\"Vn-index\", all_sentences[i], re.IGNORECASE):\n","    del all_sentences[i]\n","    del all_tokenized_sentences[i]\n","\n","print(len(all_sentences))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hluVMLOY9iND","colab_type":"text"},"source":["## Thanh khoản"]},{"cell_type":"code","metadata":{"id":"1YeUFHvT9l6X","colab_type":"code","colab":{}},"source":["import re\n","\n","for i in range(len(all_sentences) - 1, -1, -1):\n","  if not re.search(\"Thanh khoản\", all_sentences[i], re.IGNORECASE):\n","    del all_sentences[i]\n","    del all_tokenized_sentences[i]\n","\n","print(len(all_sentences))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c7JC2z9b9sWd","colab_type":"text"},"source":["## Mã cổ phiếu"]},{"cell_type":"code","metadata":{"id":"O1JplQOV9wSA","colab_type":"code","colab":{}},"source":["import re\n","\n","for i in range(len(all_sentences) - 1, -1, -1):\n","  # Chứa 3 chữ hoa liên tiếp (độc lập).\n","  # Ngoại lệ: HCM, HNX, USD\n","  if not re.search(\"(\\s[A-Z]{3}\\s)|(\\s[A-Z]{3}[^\\w])|(\\s[A-Z]{3}$)|(^[A-Z]{3}$)\", \n","                   all_sentences[i].\n","                   replace(\"HCM\", \"\").\n","                   replace(\"USD\", \"\").\n","                   replace(\"HNX\", \"\")):\n","    del all_sentences[i]\n","    del all_tokenized_sentences[i]\n","\n","print(len(all_sentences))\n","for i in range (100):\n","  print(all_sentences[i])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mI_oZRd_1dRb","colab_type":"text"},"source":["# Prepare word embedding model, stop word list"]},{"cell_type":"code","metadata":{"id":"D9g6uYEoLb7s","colab_type":"code","outputId":"7b0d7741-307e-4cbb-8e5f-140e890964d6","executionInfo":{"status":"ok","timestamp":1584981518452,"user_tz":-420,"elapsed":28265,"user":{"displayName":"Dat Trinh Tuan","photoUrl":"","userId":"16424334760389233527"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["WORD_MODEL = CLASSIFICATION_FOLDER_NAME + \"01Vnepxress.stock.model.bin\"\n","STOP_WORD_FILE = CLASSIFICATION_FOLDER_NAME + 'vietnamese-stopwords.txt'\n","\n","from gensim.models.word2vec import Word2Vec\n","from gensim.models import KeyedVectors\n","\n","my_word_model = ModelTrain(all_tokenized_sentences, file_path_to_save_model=WORD_MODEL).train()\n","# my_word_model = KeyedVectors.load_word2vec_format(WORD_MODEL, binary=True)\n","\n","stop_words = [word.strip().replace(\" \", \"_\").lower() for word in open(STOP_WORD_FILE , 'r').readlines()]"],"execution_count":14,"outputs":[{"output_type":"stream","text":["\n","Training word2vec...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:143: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n","/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"},{"output_type":"stream","text":["Result embedding shape: (5678, 200)\n","--- 21.97386360168457 seconds ---\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xB9qYKwS1qWZ","colab_type":"text"},"source":["# Thực thi - Execution"]},{"cell_type":"code","metadata":{"id":"IZMUxV8xV6da","colab_type":"code","colab":{}},"source":["a = SentenceClassification(all_sentences, all_tokenized_sentences, my_word_model, stop_words, remove_stop_words=False)\n","\n","start_time = time.time()\n","level = [100, 8]\n","clusters = a.classify(level)\n","print(\"--- %s seconds --- IN TOTAL\" % (time.time() - start_time))\n","\n","f = open(CLASSIFICATION_FOLDER_NAME + \"02Vnexpress.Sentences.Classified3.txt\", \"w+\")\n","for key in clusters[\"keys\"][len(level) - 1]:\n","  f.write(\"\\n\\nNHÓM %s\\n\" % key)\n","  for i in clusters[key]:\n","    f.write(\"%d: %s\\n\" % (i, all_sentences[i]))\n","f.close()"],"execution_count":0,"outputs":[]}]}